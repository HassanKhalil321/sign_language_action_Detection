{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgEn7KFsMNF1"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d risangbaskoro/wlasl-processed\n",
        "!unzip /content/wlasl-processed.zip -d /content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/WLASL_v0.3.json', 'r') as f:\n",
        "  data = json.load(f)"
      ],
      "metadata": {
        "id": "jABSpxAYMRNB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "id": "2SIBsVWCMWv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "\n",
        "mp_holistic = mp.solutions.holistic\n",
        "holistic_model = mp_holistic.Holistic(static_image_mode=False, min_detection_confidence=0.5)\n",
        "\n",
        "def extract_landmarks_and_draw(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        return None\n",
        "\n",
        "    full_vid = []\n",
        "    frame_landmarks = []\n",
        "    frame_count = 0\n",
        "\n",
        "    while cap.isOpened() and frame_count < 25:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = holistic_model.process(image_rgb)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            for landmark in results.pose_landmarks.landmark:\n",
        "                x = int(landmark.x * frame.shape[1])\n",
        "                y = int(landmark.y * frame.shape[0])\n",
        "                cv2.circle(frame, (x, y), 5, (0, 0, 255), 5)\n",
        "\n",
        "            connections = mp_holistic.POSE_CONNECTIONS\n",
        "            for connection in connections:\n",
        "                start_index = connection[0]\n",
        "                end_index = connection[1]\n",
        "                start_landmark = results.pose_landmarks.landmark[start_index]\n",
        "                end_landmark = results.pose_landmarks.landmark[end_index]\n",
        "                start_point = (int(start_landmark.x * frame.shape[1]), int(start_landmark.y * frame.shape[0]))\n",
        "                end_point = (int(end_landmark.x * frame.shape[1]), int(end_landmark.y * frame.shape[0]))\n",
        "                cv2.line(frame, start_point, end_point, (0, 255, 0), 1)\n",
        "\n",
        "        resized_frame = cv2.resize(frame, (640, 640))\n",
        "\n",
        "        frame_landmarks.append(resized_frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    full_vid.append(frame_landmarks)\n",
        "    cap.release()\n",
        "    return full_vid\n",
        "\n",
        "train_gloss = []\n",
        "train_instances = []\n",
        "class_instances_count = {}\n",
        "\n",
        "for train_data in data[5:7]:\n",
        "    class_name = train_data['gloss']\n",
        "    if class_name not in class_instances_count:\n",
        "        class_instances_count[class_name] = 0\n",
        "\n",
        "    for train_g in train_data['instances']:\n",
        "        if class_instances_count[class_name] >= 10:\n",
        "            break\n",
        "\n",
        "        landmarks_images = extract_landmarks_and_draw('/content/videos/' + train_g['video_id'] + '.mp4')\n",
        "        if landmarks_images is None:\n",
        "            continue\n",
        "\n",
        "        train_gloss.append(class_name)\n",
        "        train_instances.extend(landmarks_images)\n",
        "        class_instances_count[class_name] += 1\n"
      ],
      "metadata": {
        "id": "b8MNjCWWMYKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i, video_frames in enumerate(train_instances):\n",
        "    for j, frame in enumerate(video_frames):\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(frame[:, :, ::-1])\n",
        "        plt.title(f\"Frame {j+1} of video {train_gloss[i]}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "FKVOY8Q-RrbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "augmented_vids=[]\n",
        "augmented_gloss=[]\n",
        "\n",
        "\n",
        "Image_data_generator1 = ImageDataGenerator(zoom_range=0.3)\n",
        "Image_data_generator2 = ImageDataGenerator(height_shift_range=0.3)\n",
        "\n",
        "\n",
        "for index , vids in enumerate(train_instances):\n",
        "  augmented_frames1=[]\n",
        "  augmented_frames2=[]\n",
        "\n",
        "  for frames in vids:\n",
        "    augmented_frames1.append(Image_data_generator1.random_transform(frames))\n",
        "    augmented_frames2.append(Image_data_generator2.random_transform(frames))\n",
        "\n",
        "\n",
        "  augmented_vids.append(augmented_frames1)\n",
        "  augmented_vids.append(augmented_frames2)\n",
        "\n",
        "  augmented_gloss.append(train_gloss[index])\n",
        "  augmented_gloss.append(train_gloss[index])\n",
        "\n",
        "train_data = train_instances + augmented_vids\n",
        "train_labels = train_gloss + augmented_gloss"
      ],
      "metadata": {
        "id": "QGMLZaN-BZG5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "id": "JQcv8vfzF13P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_labels)"
      ],
      "metadata": {
        "id": "9p33u48mIp7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_vids=[]\n",
        "import tensorflow as tf\n",
        "for vids in train_data:\n",
        "  train_instances_tensor = tf.convert_to_tensor(vids)\n",
        "  train_vids.append(train_instances_tensor)\n"
      ],
      "metadata": {
        "id": "1TcJPkJaMlOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_vids)"
      ],
      "metadata": {
        "id": "Du6y6I2aKQmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_gloss_numerical = label_encoder.fit_transform(train_labels)\n",
        "\n",
        "train_gloss_one_hot = to_categorical(train_gloss_numerical)\n"
      ],
      "metadata": {
        "id": "2AEarCMGMq-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "    while np.array(train_data[i]).shape[0] < 25:\n",
        "        train_data[i] = np.append(train_data[i], np.zeros((1, 224, 224, 3)), axis=0)\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_data = train_data / 255.0\n"
      ],
      "metadata": {
        "id": "QrSwT-OHMtAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data, train_gloss_one_hot, test_size=0.2, random_state=42,shuffle=True)\n"
      ],
      "metadata": {
        "id": "O0BBAs2-QFgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 1\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(batch_size)\n"
      ],
      "metadata": {
        "id": "CNijXLWtMufa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "inputs = layers.Input(shape=(25, 640, 640, 3))\n",
        "\n",
        "x = layers.Conv3D(32, (3, 3, 3), activation='relu')(inputs)\n",
        "x = layers.MaxPooling3D((2, 2, 2))(x)\n",
        "x = layers.Conv3D(64, (3, 3, 3), activation='relu')(x)\n",
        "x = layers.MaxPooling3D((2, 2, 2))(x)\n",
        "x = layers.Conv3D(128, (3, 3, 3), activation='relu')(x)\n",
        "x = layers.MaxPooling3D((2, 2, 2))(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_dataset, epochs=50, validation_data=test_dataset)\n"
      ],
      "metadata": {
        "id": "7XWeushVMwM5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}